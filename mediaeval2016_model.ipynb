{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gpubert_base_mediaeval2016 (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2ElsnSNUridI",
        "pNTKJF2OhUZL",
        "q2079Qyn8Mt8",
        "LVuLV5f71502",
        "W2h73ddGXBiN"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHgSmyYN25jO"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5qCiuIn23OL",
        "outputId": "5dba06af-a03e-4acf-eba1-a9cc17538be4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElsnSNUridI"
      },
      "source": [
        "# Install packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NmMdkZO8R6q",
        "outputId": "4a1bebe7-cf59-4d8b-db7c-4ec297e2c2b8"
      },
      "source": [
        "!pip install -q transformers\n",
        "# !pip install -q tensorflow==2.2-rc1\n",
        "!pip install -q tf-models-official==2.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.8MB 5.6MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.9MB 53.2MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 60.3MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 716kB 4.3MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 11.8MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37.6MB 79kB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102kB 11.4MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2MB 45.0MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 174kB 75.1MB/s \n",
            "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ_8mMli_1sD",
        "outputId": "39739a92-2d6b-432d-a8a6-ba2891de5706"
      },
      "source": [
        "!pip install keras-lr-multiplier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-lr-multiplier\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/78/0eed4862a7274fb491b50881dd2f0dac996ff5774dc4a30c4b628fb78b25/keras-lr-multiplier-0.8.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-lr-multiplier) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-lr-multiplier) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-lr-multiplier) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-lr-multiplier) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-lr-multiplier) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras-lr-multiplier) (1.15.0)\n",
            "Building wheels for collected packages: keras-lr-multiplier\n",
            "  Building wheel for keras-lr-multiplier (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-lr-multiplier: filename=keras_lr_multiplier-0.8.0-cp36-none-any.whl size=5717 sha256=2684c3f0ecc643378a7edece533e76a24e1201f165e10c3c2573b95b2e17292e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a5/a4/340d5432bced221b2bcca324e3257239784dd1220ab7c786e9\n",
            "Successfully built keras-lr-multiplier\n",
            "Installing collected packages: keras-lr-multiplier\n",
            "Successfully installed keras-lr-multiplier-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQUy9Tat2EF_"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F_Bi5dp3LsP"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TFAutoModel, AutoTokenizer, TFBertForSequenceClassification,AutoConfig\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Maximum,LayerNormalization,GlobalMaxPooling2D,Average,Dot, Dense, Input, GlobalAveragePooling1D, BatchNormalization, Activation, Concatenate, Flatten, Dropout, Conv1D, MaxPooling1D, Add, Lambda, GlobalAveragePooling2D, Reshape, RepeatVector, UpSampling1D \n",
        "from tensorflow.keras.models import Model\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "from official import nlp\n",
        "import official.nlp.optimization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8xIUsmlw2ba"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3wfp9Vww10Q"
      },
      "source": [
        "# Set seed value\n",
        "seed_value = 56\n",
        "os.environ['PYTHONHASHSEED']=str(0)\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "random.seed(seed_value)\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "np.random.seed(seed_value)\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "tf.random.set_seed(seed_value)\n",
        "# for later versions: \n",
        "# tf.compat.v1.set_random_seed(seed_value)\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "# K.set_session(sess)\n",
        "# for later versions:\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2H2gCoL4b-_"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUto8PzE4dk8"
      },
      "source": [
        "base_dir    = '/content/drive/MyDrive/mediaeval2016'\n",
        "train_path  = os.path.join(base_dir, 'train-ekphrasis.csv')\n",
        "val_path    = os.path.join(base_dir, 'dev-ekphrasis.csv')\n",
        "test_path    = os.path.join(base_dir, 'test-ekphrasis.csv')\n",
        "img_train_path = os.path.join(base_dir, 'train-img-224.npy')\n",
        "img_val_path = os.path.join(base_dir, 'dev-img-224.npy')\n",
        "img_test_path = os.path.join(base_dir, 'test-img-224.npy')\n",
        "MAX_LENGTH  = 32\n",
        "MODEL       = 'vinai/bertweet-base'\n",
        "MODEL_NAME  = 'vinai/bertweet-base'\n",
        "N_LABELS    = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0QV2ntMpuBP"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "_UkeC7SG2krJ",
        "outputId": "cbbd9cce-05d8-4924-f40f-c517321e6949"
      },
      "source": [
        "df_train = pd.read_csv(train_path)\n",
        "print(df_train.shape)\n",
        "print(df_train.info())\n",
        "\n",
        "display(df_train.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12376, 9)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12376 entries, 0 to 12375\n",
            "Data columns (total 9 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   post_id       12376 non-null  int64 \n",
            " 1   post_text     12376 non-null  object\n",
            " 2   user_id       12376 non-null  int64 \n",
            " 3   image_id(s)   12376 non-null  object\n",
            " 4   username      12376 non-null  object\n",
            " 5   timestamp     12376 non-null  object\n",
            " 6   label         12376 non-null  object\n",
            " 7   cleaned_text  12376 non-null  object\n",
            " 8   enc_label     12376 non-null  int64 \n",
            "dtypes: int64(3), object(6)\n",
            "memory usage: 870.3+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>post_text</th>\n",
              "      <th>user_id</th>\n",
              "      <th>image_id(s)</th>\n",
              "      <th>username</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>enc_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>264364835565883393</td>\n",
              "      <td>Dear mainstream media, Sandy fucked us up too....</td>\n",
              "      <td>366606850</td>\n",
              "      <td>sandyB_real_10</td>\n",
              "      <td>NevenaBK</td>\n",
              "      <td>Fri Nov 02 13:54:23 +0000 2012</td>\n",
              "      <td>real</td>\n",
              "      <td>Dear mainstream media , Sandy fucked us up too...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>262962084801814528</td>\n",
              "      <td>Crazy pic taken in #NewYork today! #Hurricane ...</td>\n",
              "      <td>47332641</td>\n",
              "      <td>sandyA_fake_29</td>\n",
              "      <td>iChrisHarrison</td>\n",
              "      <td>Mon Oct 29 17:00:21 +0000 2012</td>\n",
              "      <td>fake</td>\n",
              "      <td>Crazy pic taken in #NewYork today ! #Hurricane...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>264415086981038080</td>\n",
              "      <td>Hurricane A-rod #mlbmemes http://t.co/1cyx4OdM</td>\n",
              "      <td>63635369</td>\n",
              "      <td>sandyB_real_10</td>\n",
              "      <td>wiegman_tommy</td>\n",
              "      <td>Fri Nov 02 17:14:03 +0000 2012</td>\n",
              "      <td>real</td>\n",
              "      <td>Hurricane A - rod #mlbmemes &lt;url&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>591987644134887425</td>\n",
              "      <td>Nepal's historic Dharahara Tower collapses in ...</td>\n",
              "      <td>485782675</td>\n",
              "      <td>nepal_25</td>\n",
              "      <td>EliseArja</td>\n",
              "      <td>Sat Apr 25 15:30:27 +0000 2015</td>\n",
              "      <td>real</td>\n",
              "      <td>Nepal ' s historic Dharahara Tower collapses i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>263997194540032003</td>\n",
              "      <td>O furac√£o Sandy passou por Nova Iorque e destr...</td>\n",
              "      <td>354373541</td>\n",
              "      <td>sandyA_real_48</td>\n",
              "      <td>GALOUCURACORINT</td>\n",
              "      <td>Thu Nov 01 13:33:30 +0000 2012</td>\n",
              "      <td>real</td>\n",
              "      <td>Hurricane Sandy passed through New York and de...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              post_id  ... enc_label\n",
              "0  264364835565883393  ...         0\n",
              "1  262962084801814528  ...         1\n",
              "2  264415086981038080  ...         0\n",
              "3  591987644134887425  ...         0\n",
              "4  263997194540032003  ...         0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "blqIvQaQncdJ",
        "outputId": "bcd7202c-0562-4f5b-8b7e-75658f339eed"
      },
      "source": [
        "df_val = pd.read_csv(val_path)\n",
        "print(df_val.shape)\n",
        "print(df_val.info())\n",
        "\n",
        "display(df_val.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1386, 9)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1386 entries, 0 to 1385\n",
            "Data columns (total 9 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   post_id       1386 non-null   int64 \n",
            " 1   post_text     1386 non-null   object\n",
            " 2   user_id       1386 non-null   int64 \n",
            " 3   image_id(s)   1386 non-null   object\n",
            " 4   username      1386 non-null   object\n",
            " 5   timestamp     1386 non-null   object\n",
            " 6   label         1386 non-null   object\n",
            " 7   cleaned_text  1386 non-null   object\n",
            " 8   enc_label     1386 non-null   int64 \n",
            "dtypes: int64(3), object(6)\n",
            "memory usage: 97.6+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>post_text</th>\n",
              "      <th>user_id</th>\n",
              "      <th>image_id(s)</th>\n",
              "      <th>username</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>enc_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>263067887546281984</td>\n",
              "      <td>Hurricane #Sandy, by @time http://t.co/ixAeykg1</td>\n",
              "      <td>112749098</td>\n",
              "      <td>sandyA_real_22</td>\n",
              "      <td>amelieamaral</td>\n",
              "      <td>Tue Oct 30 00:00:46 +0000 2012</td>\n",
              "      <td>real</td>\n",
              "      <td>Hurricane #Sandy , by &lt;user&gt; &lt;url&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>263012006943158274</td>\n",
              "      <td>That shit cray.. #sandy http://t.co/PmI0RKvq</td>\n",
              "      <td>322326492</td>\n",
              "      <td>sandyA_fake_29</td>\n",
              "      <td>HoZe_B1</td>\n",
              "      <td>Mon Oct 29 20:18:43 +0000 2012</td>\n",
              "      <td>fake</td>\n",
              "      <td>That shit cray . . #sandy &lt;url&gt;</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>263218608790970369</td>\n",
              "      <td>The effects of hurricane sandy in Harlem New Y...</td>\n",
              "      <td>30071828</td>\n",
              "      <td>sandyA_real_20</td>\n",
              "      <td>AKidNameTraveee</td>\n",
              "      <td>Tue Oct 30 09:59:41 +0000 2012</td>\n",
              "      <td>real</td>\n",
              "      <td>The effects of hurricane sandy in Harlem New Y...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>263098784236597248</td>\n",
              "      <td>O.O #Jesuuuu RT @Famosos_RD: Este tiburon no s...</td>\n",
              "      <td>180451067</td>\n",
              "      <td>sandyA_fake_32</td>\n",
              "      <td>edluis_11</td>\n",
              "      <td>Tue Oct 30 02:03:32 +0000 2012</td>\n",
              "      <td>fake</td>\n",
              "      <td>O. O #Jesuuuu RT &lt;user&gt;: This shark cannot com...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>264017358262243328</td>\n",
              "      <td>\\\"@TuiterHits: IMPRESIONANTE: Las inundaciones...</td>\n",
              "      <td>419835315</td>\n",
              "      <td>sandyA_fake_12</td>\n",
              "      <td>elenitacanon15</td>\n",
              "      <td>Thu Nov 01 14:53:38 +0000 2012</td>\n",
              "      <td>fake</td>\n",
              "      <td>\\ \"&lt;user&gt;: AWESOME: Flooding from Hurricane Sa...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              post_id  ... enc_label\n",
              "0  263067887546281984  ...         0\n",
              "1  263012006943158274  ...         1\n",
              "2  263218608790970369  ...         0\n",
              "3  263098784236597248  ...         1\n",
              "4  264017358262243328  ...         1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "jsVG3b3cjoN-",
        "outputId": "aa0dc52c-7a34-4aed-d5a9-c2dc82a17ea2"
      },
      "source": [
        "df_test = pd.read_csv(test_path)\n",
        "print(df_test.shape)\n",
        "print(df_test.info())\n",
        "\n",
        "display(df_test.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1117, 9)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1117 entries, 0 to 1116\n",
            "Data columns (total 9 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   post_id       1117 non-null   int64  \n",
            " 1   post_text     1117 non-null   object \n",
            " 2   user_id       1117 non-null   float64\n",
            " 3   username      1117 non-null   object \n",
            " 4   image_id      1117 non-null   object \n",
            " 5   timestamp     1117 non-null   object \n",
            " 6   label         1117 non-null   object \n",
            " 7   cleaned_text  1117 non-null   object \n",
            " 8   enc_label     1117 non-null   int64  \n",
            "dtypes: float64(1), int64(2), object(6)\n",
            "memory usage: 78.7+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>post_text</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>image_id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>enc_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>665333038944002048</td>\n",
              "      <td>Tristesse...üò¢üôè \\n#Bataclan sold out, musiciens...</td>\n",
              "      <td>5.547267e+08</td>\n",
              "      <td>Louise_Officiel</td>\n",
              "      <td>attacks_paris_1</td>\n",
              "      <td>Sat Nov 14 00:58:52 +0000 2015</td>\n",
              "      <td>fake</td>\n",
              "      <td>Sadness. . . üò¢ üôè #Bataclan sold out, public mu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>665324167785410560</td>\n",
              "      <td>RT @Proyecto40: #√öltimaHora Espectacular fotog...</td>\n",
              "      <td>2.827010e+09</td>\n",
              "      <td>MarinoCarril</td>\n",
              "      <td>attacks_paris_1</td>\n",
              "      <td>Sat Nov 14 00:23:37 +0000 2015</td>\n",
              "      <td>fake</td>\n",
              "      <td>&lt;user&gt;: # UltimaHora Spectacular photograph ta...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>665333370205765632</td>\n",
              "      <td>RT @Javivi1976: #Bataclan esta noche antes de ...</td>\n",
              "      <td>3.776037e+08</td>\n",
              "      <td>MireiaMaroto</td>\n",
              "      <td>attacks_paris_1</td>\n",
              "      <td>Sat Nov 14 01:00:11 +0000 2015</td>\n",
              "      <td>fake</td>\n",
              "      <td>&lt;user&gt;: #Bataclan tonight before the attacks. ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>665326188735295489</td>\n",
              "      <td>RT @Pizzigatas: El hombre tiene que establecer...</td>\n",
              "      <td>1.102841e+08</td>\n",
              "      <td>maria_cabrera</td>\n",
              "      <td>attacks_paris_1</td>\n",
              "      <td>Sat Nov 14 00:31:39 +0000 2015</td>\n",
              "      <td>fake</td>\n",
              "      <td>&lt;user&gt;: Man has to establish an end to the war...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>665389341141659648</td>\n",
              "      <td>üá´üá∑ #Paris https://t.co/zjjRPC7USm</td>\n",
              "      <td>1.653118e+07</td>\n",
              "      <td>RaycMolina</td>\n",
              "      <td>attacks_paris_1</td>\n",
              "      <td>Sat Nov 14 04:42:35 +0000 2015</td>\n",
              "      <td>fake</td>\n",
              "      <td>üá´ üá∑ #Paris &lt;url&gt;</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              post_id  ... enc_label\n",
              "0  665333038944002048  ...         1\n",
              "1  665324167785410560  ...         1\n",
              "2  665333370205765632  ...         1\n",
              "3  665326188735295489  ...         1\n",
              "4  665389341141659648  ...         1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "train_sent      = df_train.cleaned_text.values\n",
        "train_labels    = df_train.enc_label.values\n",
        "val_sent        = df_val.cleaned_text.values\n",
        "val_labels      = df_val.enc_label.values \n",
        "test_sent        = df_test.cleaned_text.values\n",
        "test_labels      = df_test.enc_label.values \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4LV-Gn4sUUp"
      },
      "source": [
        "#Bertweet tokens\n",
        "import re\n",
        "\n",
        "for i in range(train_sent.shape[0]):\n",
        "  train_sent[i] = re.sub(r'<url>','HTTPURL',train_sent[i])\n",
        "  train_sent[i] = re.sub(r'<user>','@USER',train_sent[i])\n",
        "for i in range(val_sent.shape[0]):\n",
        "  val_sent[i] = re.sub(r'<url>','HTTPURL',val_sent[i])\n",
        "  val_sent[i] = re.sub(r'<user>','@USER',val_sent[i])\n",
        "for i in range(test_sent.shape[0]):\n",
        "  test_sent[i] = re.sub(r'<url>','HTTPURL',test_sent[i])\n",
        "  test_sent[i] = re.sub(r'<user>','@USER',test_sent[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-R_39QVsVlS"
      },
      "source": [
        "train_sent = np.append(train_sent,val_sent)\n",
        "train_labels = np.append(train_labels,val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hraBqZt23Q7n",
        "outputId": "9deb97a5-4e72-49e6-8eaa-d5ec25032560"
      },
      "source": [
        "#Image\n",
        "img_train = np.load(img_train_path)\n",
        "img_val = np.load(img_val_path)\n",
        "img_test = np.load(img_test_path)\n",
        "img_train.shape,  img_test.shape, img_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((12376, 224, 224, 3), (1117, 224, 224, 3), (1386, 224, 224, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK04cChTsafQ",
        "outputId": "0cf2f596-730a-439e-fd54-90da7ebd1930"
      },
      "source": [
        "#Use both train and val to train like other papers\n",
        "img_train = np.vstack((img_train,img_val))\n",
        "img_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13762, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6XvCkqT7wxO",
        "outputId": "d4f04ce9-1568-45cf-85a6-a30c0533be00"
      },
      "source": [
        "print(len(train_sent), len(train_labels))\n",
        "print(len(val_sent), len(val_labels))\n",
        "print(len(test_sent), len(test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13762 13762\n",
            "1386 1386\n",
            "1117 1117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "# Tokenization & Input Formatting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSNdVzzBsoFQ",
        "outputId": "33f8e772-d6c6-4aa6-ffd3-9dda6130542d"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z474sSC6oe7A",
        "outputId": "d4abb00d-49f1-439a-b3d1-f21c347676b3"
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLIbudgfh6F0",
        "outputId": "e176bbcd-c3ef-4131-a019-42f9e2e45b19"
      },
      "source": [
        "print(' Original: ', train_sent[4])\n",
        "print('Tokenized: ', tokenizer.tokenize(train_sent[4]))\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sent[4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Hurricane Sandy passed through New York and destroyed everything, or rather, almost everything. See this BBC photo. Virgin Mary . . . HTTPURL \n",
            "Tokenized:  ['Hurricane', 'Sandy', 'passed', 'through', 'New', 'York', 'and', 'destroyed', 'everything@@', ',', 'or', 'ra@@', 'ther@@', ',', 'almost', 'everything@@', '.', 'See', 'this', 'BBC', 'photo@@', '.', 'Virgin', 'Mary', '.', '.', '.', 'HTTPURL']\n",
            "Token IDs:  [11623, 11741, 2262, 292, 210, 1092, 13, 5537, 42345, 7, 72, 1127, 3270, 7, 683, 42345, 4, 740, 33, 2674, 9844, 4, 11342, 3555, 4, 4, 4, 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viKGCCh8izww"
      },
      "source": [
        "## Find max length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKsH2sU0OCQA",
        "outputId": "6fad5d0e-b59c-45f2-c087-0276beeb930d"
      },
      "source": [
        "max_len = 0\n",
        "stat = []\n",
        "for sent in tqdm(train_sent):\n",
        "    input_ids   = tokenizer.encode(sent, add_special_tokens=True)\n",
        "#    if(len(input_ids)>96):\n",
        "#      print(sent)\n",
        "    max_len     = max(max_len, len(input_ids))\n",
        "    stat.append(len(input_ids))\n",
        "\n",
        "print('\\nMax sentence length in train data: ', max_len)\n",
        "\n",
        "# for sent in tqdm(val_sent):\n",
        "#     input_ids   = tokenizer.encode(sent, add_special_tokens=True)\n",
        "#  #   if(len(input_ids)>96):\n",
        "#  #     print(sent)    \n",
        "#     max_len     = max(max_len, len(input_ids))\n",
        "#     stat.append(len(input_ids))\n",
        "\n",
        "# print('\\nMax sentence length in both train and val data: ', max_len)\n",
        "\n",
        "for index,sent in tqdm(enumerate(test_sent)):\n",
        "   input_ids   = tokenizer.encode(sent, add_special_tokens=True)\n",
        "   if(len(input_ids)>96):\n",
        "     print(index, sent)\n",
        "   max_len     = max(max_len, len(input_ids))\n",
        "   stat.append(len(input_ids))\n",
        "\n",
        "print('\\nMax sentence length in both train and val data: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13762/13762 [00:01<00:00, 6951.94it/s]\n",
            "1117it [00:00, 6148.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Max sentence length in train data:  101\n",
            "\n",
            "Max sentence length in both train and val data:  101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaunFPWIFZzJ"
      },
      "source": [
        "## Tokenize "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjademTSD0VA"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bBdb3pt8LuQ",
        "outputId": "01a3e3da-0ba6-4ce9-abc2-2c5b284ca5b8"
      },
      "source": [
        "input_ids       = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in tqdm(train_sent):\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = MAX_LENGTH,         \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'np',   \n",
        "                        truncation = True,\n",
        "                   )\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  \n",
        "id_train        = np.concatenate(input_ids)\n",
        "mask_train      = np.concatenate(attention_masks)\n",
        "y_train         = train_labels \n",
        "id_train.shape, mask_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/13762 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13762/13762 [00:01<00:00, 7031.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13762, 32), (13762, 32), (13762,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neekd6auD2Zf"
      },
      "source": [
        "### Val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEgLpFVlo1Z-",
        "outputId": "c142d7a7-270e-47bb-f38f-53b28a52ab37"
      },
      "source": [
        "input_ids       = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in tqdm(val_sent):\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = MAX_LENGTH,         \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'np',   \n",
        "                        truncation = True,\n",
        "                   )\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "\n",
        "id_val          = np.concatenate(input_ids)\n",
        "mask_val        = np.concatenate(attention_masks)\n",
        "y_val           = val_labels\n",
        "id_val.shape, mask_val.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1386 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1386/1386 [00:00<00:00, 6899.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1386, 32), (1386, 32), (1386,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvidOCpCAtQ4"
      },
      "source": [
        "## Create iterator for data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGUqOCtgqGhP"
      },
      "source": [
        "BATCH_SIZE      = 256 \n",
        "\n",
        "X_train         = [\n",
        "    id_train,\n",
        "    mask_train,\n",
        "    img_train\n",
        "]\n",
        "X_val           = [\n",
        "    id_val,\n",
        "    mask_val,\n",
        "    img_val  \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwa6Rts-02-"
      },
      "source": [
        "# Train TFBertForSequenceClassification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvdfkSl1YZto"
      },
      "source": [
        "from tensorflow.keras.applications import VGG19                                  #Change here for different models\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input                 #Change here for different models\n",
        "\n",
        "def create_cnn(input_shape):\n",
        "  inputs = Input(shape=input_shape)\n",
        "  x = Lambda(preprocess_input)(inputs)\n",
        "  # load the VGG16 network, ensuring the head FC layer sets are left\n",
        "  # off                                                                           - Change here for different models\n",
        "  baseModel = VGG19(weights=\"imagenet\", include_top=True, input_tensor=x)\n",
        "\n",
        "  # construct the head of the model that will be placed on top of the\n",
        "  # the base model                                           \n",
        "  headModel_cnn = baseModel.layers[22].output\n",
        "  headModel_fc  = baseModel.layers[25].output\n",
        "  ###Attention\n",
        "  headModel_cnn = Reshape((-1,headModel_cnn.shape[-1]))(headModel_cnn)\n",
        "\n",
        "  ###Non-attention\n",
        "  # headModel = Flatten()(headModel)                                 \n",
        "  # headModel = Dense(512)(headModel)\n",
        "  # headModel = BatchNormalization()(headModel)\n",
        "  # headModel = Activation(\"relu\")(headModel)\n",
        "  # headModel = Dropout(0.2)(headModel)\n",
        "\n",
        "  # headModel = Dense(512)(headModel)\n",
        "  # headModel = BatchNormalization()(headModel)\n",
        "  # headModel = Activation(\"relu\")(headModel)\n",
        "  # headModel = Dropout(0.2)(headModel)\n",
        "\n",
        "  model = Model(inputs=baseModel.input, outputs=[headModel_cnn, headModel_fc])\n",
        "\n",
        "  # loop over all layers in the base model and freeze them so they will\n",
        "  # *not* be updated during the first training process\n",
        "  for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhrTo5SfruVb"
      },
      "source": [
        "def create_model(transformer, max_len=256):\n",
        "    merge = []\n",
        "\n",
        "    input_ids           = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask      = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
        "    image               = create_cnn((224,224,3))\n",
        "    sequence_output_1     = transformer(input_ids, attention_mask=attention_mask)[2][-1]\n",
        "    sequence_output_2     = transformer(input_ids, attention_mask=attention_mask)[2][-2]\n",
        "    sequence_output_3     = transformer(input_ids, attention_mask=attention_mask)[2][-3]\n",
        "    sequence_output_4     = transformer(input_ids, attention_mask=attention_mask)[2][-4]\n",
        "    sequence_output       = Concatenate(axis=2)([sequence_output_1, sequence_output_2, sequence_output_3, sequence_output_4])\n",
        "\n",
        "   #cls_token           = sequence_output[:, 0, :]\n",
        "    \n",
        "    #merge.append(cls_token)\n",
        "  \n",
        "   # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
        "    convs = []\n",
        "    filter_sizes = [2,3,4,5]\n",
        "    size_pool = 3\n",
        "    drop_rate = 0.2\n",
        "    final_hid = 32\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=768, kernel_size=filter_size)(sequence_output)\n",
        "        l_conv = BatchNormalization()(l_conv)\n",
        "        l_conv = Activation('relu')(l_conv)\n",
        "        # l_pool = MaxPooling1D(pool_size=max_len-filter_size+1)(l_conv)\n",
        "        l_pool = MaxPooling1D(pool_size=size_pool)(l_conv)\n",
        "        convs.append(l_pool)\n",
        "        #merge.append(Flatten()(l_pool))\n",
        "\n",
        "    l2_pool = Concatenate(axis=1)(convs)\n",
        "    # l2_pool = BatchNormalization()(l2_pool)\n",
        "    for _ in range(2):\n",
        "        origin  = l2_pool\n",
        "        l2_conv = Conv1D(filters=768, kernel_size=size_pool,padding='same')(l2_pool)\n",
        "        l2_conv = BatchNormalization()(l2_conv)\n",
        "        l2_conv = Activation('relu')(l2_conv)\n",
        "        #print(origin.shape, l2_conv.shape)\n",
        "        # l2_conv = Add()([Lambda(lambda x: x[0]*x[1])([origin,0.1]), l2_conv])\n",
        "        l2_pool = MaxPooling1D(pool_size=size_pool)(l2_conv)\n",
        "\n",
        "    text = Flatten()(l2_pool)\n",
        "    #append to merge\n",
        "    text_append = Dense(512)(text)\n",
        "    text_append = BatchNormalization()(text_append)\n",
        "    text_append = Activation('relu')(text_append)\n",
        "    text_append = Dropout(drop_rate)(text_append)\n",
        "    text_append = Dense(final_hid)(text_append)\n",
        "    text_append = BatchNormalization()(text_append)\n",
        "    text_append = Activation('relu')(text_append)\n",
        "\n",
        "    # merge.append(text_append)\n",
        "    # text_append = l2_pool\n",
        "\n",
        "    #text for attention\n",
        "    # text = Dropout(0.2)(text)\n",
        "    # text = Dense(128)(text)\n",
        "    # text = BatchNormalization()(text)\n",
        "    # text = Activation(\"tanh\")(text)                  #change to tanh?\n",
        "    # text = Dropout(0.2)(text)\n",
        "    image_append = Dense(2048)(image.output[1])\n",
        "    image_append = BatchNormalization()(image_append)\n",
        "    image_append = Activation('relu')(image_append)\n",
        "    image_append = Dropout(drop_rate)(image_append)\n",
        "    image_append = Dense(final_hid)(image_append)\n",
        "    image_append = BatchNormalization()(image_append)\n",
        "    image_append = Activation('relu')(image_append)\n",
        "\n",
        "    img = Dense(final_hid)(image.output[0])\n",
        "    img = BatchNormalization()(img)\n",
        "    img = Activation('relu')(img)\n",
        "\n",
        "\n",
        "    # image_append = image.output[1]\n",
        "    # merge.append(image_append)\n",
        "    # image_append = image.output[0]\n",
        "    \n",
        "    head = 1\n",
        "    att_layers = 1\n",
        "    att_hid = 32\n",
        "    ##With attention - text on image\n",
        "    inpAttImg_key = img\n",
        "    inpAttImg_query = text_append\n",
        "    for layer in range(1):\n",
        "      att_img = []\n",
        "      concat_key = []\n",
        "      for _ in range(head):\n",
        "        img_key = Dense(att_hid/head, use_bias=False)(inpAttImg_key) #change to tanh?\n",
        "        text_query = Dense(att_hid/head, use_bias=False)(inpAttImg_query) \n",
        "        # img_key = BatchNormalization()(img_key)\n",
        "        # img_key = Activation('tanh')(img_key)\n",
        "        # text_query = BatchNormalization()(text_query)\n",
        "        # text_query = Activation('tanh')(text_query)\n",
        "\n",
        "        img_value = Dense(att_hid/head, use_bias=False)(inpAttImg_key)\n",
        "        # img_value = BatchNormalization()(img_value)\n",
        "        # img_value = Activation('tanh')(img_value)\n",
        "\n",
        "        attention = Dot(axes=(1,2))([text_query, img_key])\n",
        "        attention = Lambda(lambda x: x[0]/x[1])([attention,np.sqrt(att_hid/head)])\n",
        "        attention = Activation(\"softmax\")(attention)\n",
        "        head_att_img = Dot(axes=(1,1))([attention, img_value])\n",
        "        att_img = head_att_img\n",
        "        concat_key = img_key\n",
        "      # att_img = Concatenate(axis=1)(att_img)\n",
        "      # att_img = Dense(512, use_bias=False)(att_img)\n",
        "      # att_img = Dropout(0.3)(att_img)\n",
        "      # att_img = Add()([att_img, inpAttImg_query])\n",
        "      # att_img = LayerNormalization()(att_img)\n",
        "      # att_img2 = Dense(1024,activation='relu')(att_img)\n",
        "      # att_img2 = Dense(512)(att_img)\n",
        "      # att_img2 = Dropout(0.1)(att_img2)\n",
        "      # att_img = Add()([att_img, att_img2])\n",
        "      # att_img = LayerNormalization()(att_img)\n",
        "      att_img2 = Average()([ Activation('relu')(BatchNormalization()(Dense(final_hid)(att_img))),\n",
        "                             Activation('relu')(BatchNormalization()(Dense(final_hid)(att_img))),\n",
        "                             Activation('relu')(BatchNormalization()(Dense(final_hid)(att_img))), \n",
        "                             Activation('relu')(BatchNormalization()(Dense(final_hid)(att_img)))])      \n",
        "      att_img2 = Dense(final_hid)(att_img2)\n",
        "      att_img2 = BatchNormalization()(att_img2)\n",
        "      att_img2 = Activation('relu')(att_img2)\n",
        "      att_img2 = Dropout(drop_rate)(att_img2)\n",
        "      att_img = Add()([inpAttImg_query, att_img2])\n",
        "      att_img = LayerNormalization()(att_img)\n",
        "\n",
        "\n",
        "\n",
        "      # concat_key = Concatenate(axis=1)(concat_key)\n",
        "      # concat_key = Dense(512)(concat_key)\n",
        "      # concat_key = Dropout(0.3)(inpAttImg_key)\n",
        "      # concat_key = Add()([concat_key, inpAttImg_key])\n",
        "      # concat_key = LayerNormalization()(concat_key)\n",
        "      # concat_key2 = Dense(1024,activation='relu')(concat_key)\n",
        "      # concat_key2 = Dense(512)(concat_key)\n",
        "      # concat_key2 = Dropout(0.1)(concat_key2)\n",
        "      # concat_key = Add()([concat_key, concat_key2])\n",
        "      # concat_key = LayerNormalization()(concat_key)\n",
        "\n",
        "      inpAttImg_query = att_img\n",
        "      inpAttImg_key = concat_key\n",
        "\n",
        "    # merge.append(att_img)\n",
        "\n",
        "    ##attention image on text\n",
        "    inpAttText_key = l2_pool\n",
        "    inpAttText_query = image_append\n",
        "    for layer in range(1):\n",
        "      att_text = []\n",
        "      concat_key = []\n",
        "      for _ in range(head):\n",
        "        img_query = Dense(att_hid/head, use_bias=False)(inpAttText_query)\n",
        "    #     # # img_query = BatchNormalization()(img_query)\n",
        "    #     # # img_query = Activation('tanh')(img_query)\n",
        "        text_key = Dense(att_hid/head, use_bias=False)(inpAttText_key)\n",
        "    #     # # text_key = BatchNormalization()(text_key)\n",
        "    #     # # text_key = Activation('tanh')(text_key)\n",
        "        text_value = Dense(att_hid/head, use_bias=False)(inpAttText_key)\n",
        "        attention = Dot(axes=(1,2))([img_query, text_key])\n",
        "        attention = Lambda(lambda x: x[0]/x[1])([attention,np.sqrt(att_hid/head)])\n",
        "        attention = Activation(\"softmax\")(attention)\n",
        "        head_att_text = Dot(axes=(1,1))([attention, text_value])\n",
        "        att_text = head_att_text\n",
        "        concat_key = text_key\n",
        "      # att_text = Concatenate(axis=1)(att_text)\n",
        "      # att_text = Dense(512, use_bias=False)(att_text)\n",
        "      # att_text = Dropout(0.3)(att_text)\n",
        "      # att_text = Add()([att_text, inpAttText_query])\n",
        "      # att_text = LayerNormalization()(att_text)\n",
        "      # att_text2 = Dense(1024,activation='relu')(att_text)\n",
        "      # att_text2 = Dense(512)(att_text)\n",
        "      # att_text2 = Dropout(0.1)(att_text2)\n",
        "      # att_text = Add()([att_text, att_text2])\n",
        "      # att_text = LayerNormalization()(att_text)\n",
        "\n",
        "      # concat_key = Concatenate(axis=1)(concat_key)\n",
        "      # concat_key = Dense(512)(concat_key)\n",
        "      # concat_key = Dropout(0.3)(concat_key)\n",
        "      # concat_key = Add()([concat_key, inpAttText_key])\n",
        "      # concat_key = LayerNormalization()(concat_key)\n",
        "      # concat_key2 = Dense(1024,activation='relu')(concat_key)\n",
        "      # concat_key2 = Dense(512)(concat_key)\n",
        "      # concat_key2 = Dropout(0.1)(concat_key2)\n",
        "      # concat_key = Add()([concat_key, concat_key2])\n",
        "      # concat_key = LayerNormalization()(concat_key)\n",
        "      att_text2 = Average()([Activation('relu')(BatchNormalization()(Dense(final_hid)(att_text))),\n",
        "                             Activation('relu')(BatchNormalization()(Dense(final_hid)(att_text))),\n",
        "                             Activation('relu')(BatchNormalization()(Dense(final_hid)(att_text))), \n",
        "                             Activation('relu')(BatchNormalization()(Dense(final_hid)(att_text)))])\n",
        "      att_text2 = Dense(final_hid)(att_text2)\n",
        "      att_text2 = BatchNormalization()(att_text2)\n",
        "      att_text2 = Activation('relu')(att_text2)\n",
        "      att_text2 = Dropout(drop_rate)(att_text2)\n",
        "      att_text = Add()([inpAttText_query, att_text2])\n",
        "      att_text = LayerNormalization()(att_text)\n",
        "\n",
        "      inpAttText_query = att_text\n",
        "      inpAttText_key = concat_key\n",
        "    \n",
        "    head = 1\n",
        "    ###Self attention image\n",
        "    inpAttSelf_key = img\n",
        "    inpAttSelf_query = img\n",
        "    for layer in range(att_layers):\n",
        "      self_img = []\n",
        "      for _ in range(head):\n",
        "        self_query = Dense(att_hid/head, use_bias=False)(inpAttSelf_query)\n",
        "        self_key = Dense(att_hid/head, use_bias=False)(inpAttSelf_key)\n",
        "        self_value = Dense(att_hid/head, use_bias=False)(inpAttSelf_key)\n",
        "\n",
        "        attention = Dot(axes=(2,2))([self_query, self_key])\n",
        "        attention = Lambda(lambda x: x[0]/x[1])([attention,np.sqrt(att_hid/head)])\n",
        "        attention = Activation(\"softmax\")(attention)\n",
        "        head_att_self = Dot(axes=(2,1))([attention, self_value])\n",
        "        self_img = head_att_self\n",
        "\n",
        "      # self_img = Concatenate(axis=2)(self_img)\n",
        "      # self_img = Dense(512, use_bias=False)(self_img)\n",
        "      # self_img = Dropout(0.3)(self_img)\n",
        "      # self_img = Add()([self_img, inpAttSelf_query])\n",
        "      # self_img = LayerNormalization()(self_img)\n",
        "      # self_img2 = Dense(1024,activation='relu')(self_img)\n",
        "      # self_img2 = Dense(512)(self_img2)\n",
        "      # self_img2 = Dropout(0.3)(self_img2)\n",
        "      # self_img = Add()([self_img, self_img2])\n",
        "      # self_img = LayerNormalization()(self_img)\n",
        "      self_img2 = Average()([Activation('relu')(BatchNormalization()(Dense(final_hid)(self_img))),\n",
        "                             Activation('relu')(BatchNormalization()(Dense(final_hid)(self_img))),\n",
        "                             Activation('relu')(BatchNormalization()(Dense(final_hid)(self_img))), \n",
        "                             Activation('relu')(BatchNormalization()(Dense(final_hid)(self_img)))])      \n",
        "      self_img2 = Dense(final_hid)(self_img2)\n",
        "      self_img2 = BatchNormalization()(self_img2)\n",
        "      self_img2 = Activation('relu')(self_img2)\n",
        "      self_img2 = Dropout(drop_rate)(self_img2)\n",
        "      self_img = Add()([inpAttSelf_query, self_img2])\n",
        "      self_img = LayerNormalization()(self_img)\n",
        "\n",
        "      inpAttSelf_query = self_img\n",
        "      inpAttSelf_key = self_img\n",
        "\n",
        "\n",
        "    # merge.append(att_text)\n",
        "    self_img =  Flatten()(self_img)\n",
        "    # att_img =  Dense(32)(Flatten()(att_img))\n",
        "    # # # att_img = BatchNormalization()(att_img)\n",
        "    # att_img = Activation('relu')(att_img)\n",
        "\n",
        "    # att_text = Flatten()(att_text)\n",
        "    # # # att_text = Concatenate(axis=1)([att_text,self_img])\n",
        "    # att_text = Dense(32)(att_text)\n",
        "    # # # att_text = BatchNormalization()(att_text)\n",
        "    # att_text = Activation('relu')(att_text)\n",
        "\n",
        "    self_img =  Dense(final_hid)(self_img)\n",
        "    self_img = BatchNormalization()(self_img)\n",
        "    self_img = Activation('relu')(self_img)\n",
        "    \n",
        "    # self_img =  Dense(32)(self_img)\n",
        "    # # # self_img = BatchNormalization()(self_img)\n",
        "    # self_img = Activation('relu')(self_img)\n",
        "\n",
        "    # text_append =  Dense(512)(Flatten()(text_append))\n",
        "    # text_append = BatchNormalization()(text_append)\n",
        "    # text_append = Activation('relu')(text_append)\n",
        "\n",
        "    # image_append =  Dense(512)(Flatten()(image_append))\n",
        "    # image_append = BatchNormalization()(image_append)\n",
        "    # image_append = Activation('relu')(image_append)\n",
        "\n",
        "    # merge.append(att_img)\n",
        "    # merge.append(att_text)\n",
        "    merge.append(self_img)\n",
        "\n",
        "    # merge.append(Dense(512,activation='relu')(Flatten()(self_img)))\n",
        "    # merge.append(text_append)\n",
        "    merge.append(image_append)\n",
        "    l_merge             = Concatenate(axis=1)(merge)\n",
        "    # l_merge = image_append\n",
        "    l_merge = Dropout(drop_rate)(l_merge)\n",
        "    l_merge = Dense(final_hid)(l_merge)\n",
        "    l_merge = BatchNormalization()(l_merge)\n",
        "    l_merge = Activation('relu')(l_merge)\n",
        "    l_merge = Dropout(drop_rate)(l_merge)\n",
        "    # l_merge             = Average()(merge)   \n",
        "    out                 = Dense(N_LABELS, activation='sigmoid')(l_merge) \n",
        "    model               = Model(inputs=[input_ids, attention_mask, image.input], \n",
        "                            outputs=out)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLs72DuMODJO",
        "outputId": "b56320f8-7f96-44d4-9472-51840a7846f6"
      },
      "source": [
        "%%time\n",
        "EPOCHS          = 8\n",
        "total_steps     = len(y_train) * BATCH_SIZE\n",
        "train_data_size = len(y_train)\n",
        "steps_per_epoch = int(train_data_size / BATCH_SIZE) + 1\n",
        "num_train_steps = steps_per_epoch * EPOCHS\n",
        "# warmup_steps    = int(steps_per_epoch * 1)\n",
        "warmup_steps    = 0\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "decay_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "      initial_learning_rate=1e-4,\n",
        "      decay_steps=num_train_steps,\n",
        "      end_learning_rate=0)\n",
        "\n",
        "warmup_schedule = nlp.optimization.WarmUp(\n",
        "        initial_learning_rate=7e-5,\n",
        "        decay_schedule_fn=decay_schedule,\n",
        "        warmup_steps=warmup_steps)\n",
        "\n",
        "optimizer       = nlp.optimization.AdamWeightDecay(\n",
        "        learning_rate=warmup_schedule,\n",
        "        epsilon=1e-8,\n",
        "        exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])\n",
        "\n",
        "#Load bert4news\n",
        "# config = AutoConfig.from_pretrained('/content/drive/MyDrive/bert4news/config.json')\n",
        "# transformer = TFAutoModel.from_pretrained('/content/drive/MyDrive/bert4news/pytorch_model.bin', from_pt=True, config=config)\n",
        "\n",
        "transformer = TFAutoModel.from_pretrained(MODEL,output_attentions=False,output_hidden_states=True,return_dict =True)\n",
        "transformer.trainable = False\n",
        "model = create_model(transformer, max_len=MAX_LENGTH)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics='accuracy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at vinai/bertweet-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/bertweet-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 224, 224, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 49, 512)      0           block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 49, 32)       16416       reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 49, 32)       128         dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 49, 32)       0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 49, 32)       1024        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 49, 32)       1024        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dot_4 (Dot)                     (None, 49, 49)       0           dense_21[0][0]                   \n",
            "                                                                 dense_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 49, 49)       0           dot_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 49, 49)       0           lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 49, 32)       1024        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dot_5 (Dot)                     (None, 49, 32)       0           activation_23[0][0]              \n",
            "                                                                 dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 49, 32)       1056        dot_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 49, 32)       1056        dot_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 49, 32)       1056        dot_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 49, 32)       1056        dot_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 49, 32)       128         dense_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 49, 32)       128         dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 49, 32)       128         dense_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 49, 32)       128         dense_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 49, 32)       0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 49, 32)       0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 49, 32)       0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 49, 32)       0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_2 (Average)             (None, 49, 32)       0           activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "                                                                 activation_26[0][0]              \n",
            "                                                                 activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 49, 32)       1056        average_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 25088)        0           block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 49, 32)       128         dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "fc1 (Dense)                     (None, 4096)         102764544   flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 49, 32)       0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "fc2 (Dense)                     (None, 4096)         16781312    fc1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 49, 32)       0           activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2048)         8390656     fc2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 49, 32)       0           activation_10[0][0]              \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 2048)         8192        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_2 (LayerNor (None, 49, 32)       64          add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 2048)         0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 1568)         0           layer_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 2048)         0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 32)           50208       flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 32)           65568       dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 32)           128         dense_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32)           128         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 32)           0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32)           0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 64)           0           activation_29[0][0]              \n",
            "                                                                 activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 64)           0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 32)           2080        dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 32)           128         dense_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 32)           0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 32)           0           activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_ids (InputLayer)          [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_mask (InputLayer)     [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 1)            33          dropout_43[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 148,112,961\n",
            "Trainable params: 8,538,049\n",
            "Non-trainable params: 139,574,912\n",
            "__________________________________________________________________________________________________\n",
            "CPU times: user 5.2 s, sys: 461 ms, total: 5.66 s\n",
            "Wall time: 6.73 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LuDQbbu-3Sm"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(np.argmax(train_labels, axis=1)), np.argmax(train_labels, axis=1))\n",
        "class_weights = {i : class_weights[i] for i in range(3)}\n",
        "class_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM5WbtoiUAz2"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_CEl-sY4Trd"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "class roc_auc_callback(Callback):\n",
        "    def __init__(self,training_data,validation_data):\n",
        "        self.x = training_data[0]\n",
        "        self.y = training_data[1]\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # y_pred_train = self.model.predict(self.x, verbose=0)\n",
        "        # y_pred_train = (y_pred_train>=0.5)*1\n",
        "        # y_true_train = self.y\n",
        "        # f1_train_micro = f1_score(y_true_train, y_pred_train, average='micro')\n",
        "        # f1_train_macro = f1_score(y_true_train, y_pred_train, average='macro')\n",
        "        \n",
        "        y_pred_val = self.model.predict(self.x_val, verbose=0)\n",
        "        y_pred_val = (y_pred_val>0.5)*1\n",
        "        y_true_val = self.y_val\n",
        "        f1_val_micro = f1_score(y_true_val, y_pred_val, average='micro')\n",
        "        f1_val_macro = f1_score(y_true_val, y_pred_val, average='macro')\n",
        "        print('\\ f1_val: micro - %s macro - %s' % (str(round(f1_val_micro,4)),str(round(f1_val_macro,4))),end=100*' '+'\\n')\n",
        "        return\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktez2KF-UCVT",
        "outputId": "c1c76da0-7e84-4bc1-d548-1c5af7472185"
      },
      "source": [
        "n_steps = int(np.ceil(y_train.shape[0] / BATCH_SIZE))\n",
        "\n",
        "# Checkpoint path\n",
        "ckpt_path     = f'/content/drive/My Drive/mediaeval2016/checkpoint-test/{MODEL_NAME}/'\n",
        "if not os.path.exists(ckpt_path):\n",
        "    os.makedirs(ckpt_path)\n",
        "ckpt_path     += 'cp-{epoch:02d}.h5'\n",
        "\n",
        "# Callback\n",
        "my_callbacks  = [tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_path, \n",
        "                                                    monitor='val_loss', \n",
        "                                                    save_weights_only=True,\n",
        "                                                    save_freq='epoch'),\n",
        "                 tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
        "                 roc_auc_callback(training_data=(X_train, y_train),validation_data=(X_test_data, test_labels))]\n",
        "\n",
        "H = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test_data, test_labels),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    #steps_per_epoch=n_steps,\n",
        "    # class_weight=class_weights,\n",
        "    shuffle=False,\n",
        "    callbacks=my_callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "54/54 [==============================] - 35s 596ms/step - loss: 0.3178 - accuracy: 0.8770 - val_loss: 0.5263 - val_accuracy: 0.7287\n",
            "\\ f1_val: micro - 0.7287 macro - 0.7172                                                                                                    \n",
            "Epoch 2/8\n",
            "54/54 [==============================] - 32s 588ms/step - loss: 0.1122 - accuracy: 0.9883 - val_loss: 0.5316 - val_accuracy: 0.6992\n",
            "\\ f1_val: micro - 0.6992 macro - 0.6926                                                                                                    \n",
            "Epoch 3/8\n",
            "54/54 [==============================] - 32s 587ms/step - loss: 0.0805 - accuracy: 0.9951 - val_loss: 0.5404 - val_accuracy: 0.7019\n",
            "\\ f1_val: micro - 0.7019 macro - 0.6974                                                                                                    \n",
            "Epoch 4/8\n",
            "54/54 [==============================] - 32s 591ms/step - loss: 0.0635 - accuracy: 0.9973 - val_loss: 0.5626 - val_accuracy: 0.6947\n",
            "\\ f1_val: micro - 0.6947 macro - 0.6915                                                                                                    \n",
            "Epoch 5/8\n",
            "54/54 [==============================] - 32s 587ms/step - loss: 0.0533 - accuracy: 0.9984 - val_loss: 0.5961 - val_accuracy: 0.7278\n",
            "\\ f1_val: micro - 0.7278 macro - 0.7253                                                                                                    \n",
            "Epoch 6/8\n",
            "54/54 [==============================] - 32s 588ms/step - loss: 0.0463 - accuracy: 0.9987 - val_loss: 0.6241 - val_accuracy: 0.6016\n",
            "\\ f1_val: micro - 0.6016 macro - 0.6012                                                                                                    \n",
            "Epoch 7/8\n",
            "54/54 [==============================] - 32s 587ms/step - loss: 0.0388 - accuracy: 0.9994 - val_loss: 0.6431 - val_accuracy: 0.5864\n",
            "\\ f1_val: micro - 0.5864 macro - 0.5849                                                                                                    \n",
            "Epoch 8/8\n",
            "54/54 [==============================] - 32s 587ms/step - loss: 0.0347 - accuracy: 0.9998 - val_loss: 0.6493 - val_accuracy: 0.6285\n",
            "\\ f1_val: micro - 0.6285 macro - 0.6283                                                                                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2079Qyn8Mt8"
      },
      "source": [
        "# Saving Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9HguCcq_Gap"
      },
      "source": [
        "model.save_weights(os.path.join(base_dir, f'model/{MODEL_NAME}.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf4FcvzddoYI"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m2qXpjMwmPH",
        "outputId": "972774fd-4d93-4e71-8842-f01e7c9f9554"
      },
      "source": [
        "%%time\n",
        "EPOCHS          = EPOCHS\n",
        "total_steps     = len(y_train) * BATCH_SIZE\n",
        "train_data_size = len(y_train)\n",
        "steps_per_epoch = int(train_data_size / BATCH_SIZE) + 1\n",
        "num_train_steps = steps_per_epoch * EPOCHS\n",
        "# warmup_steps    = int(num_train_steps * 0.1)\n",
        "warmup_steps    = 0\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "decay_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "      initial_learning_rate=2e-5,\n",
        "      decay_steps=num_train_steps,\n",
        "      end_learning_rate=0)\n",
        "\n",
        "warmup_schedule = nlp.optimization.WarmUp(\n",
        "        initial_learning_rate=2e-5,\n",
        "        decay_schedule_fn=decay_schedule,\n",
        "        warmup_steps=warmup_steps)\n",
        "\n",
        "optimizer       = nlp.optimization.AdamWeightDecay(\n",
        "        learning_rate=warmup_schedule,\n",
        "        epsilon=1e-8,\n",
        "        exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])\n",
        "\n",
        "\n",
        "#Load bert4news\n",
        "# config = AutoConfig.from_pretrained('/content/drive/MyDrive/bert4news/config.json')\n",
        "# transformer = TFAutoModel.from_pretrained('/content/drive/MyDrive/bert4news/pytorch_model.bin', from_pt=True, config=config)\n",
        "\n",
        "transformer = TFAutoModel.from_pretrained(MODEL,output_attentions=False,output_hidden_states=True,return_dict =True)\n",
        "transformer.trainable = False\n",
        "model2      = create_model(transformer, max_len=MAX_LENGTH)\n",
        "model2.compile(optimizer=optimizer,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics='accuracy')\n",
        "    \n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at vinai/bertweet-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/bertweet-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 224, 224, 3)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "input_ids (InputLayer)          [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_mask (InputLayer)     [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model_3 (TFRobertaMo TFBaseModelOutputWit 134899968   input_ids[0][0]                  \n",
            "                                                                 attention_mask[0][0]             \n",
            "                                                                 input_ids[0][0]                  \n",
            "                                                                 attention_mask[0][0]             \n",
            "                                                                 input_ids[0][0]                  \n",
            "                                                                 attention_mask[0][0]             \n",
            "                                                                 input_ids[0][0]                  \n",
            "                                                                 attention_mask[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 3072)     0           tf_roberta_model_3[0][12]        \n",
            "                                                                 tf_roberta_model_3[1][11]        \n",
            "                                                                 tf_roberta_model_3[2][10]        \n",
            "                                                                 tf_roberta_model_3[3][9]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 31, 768)      4719360     concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 30, 768)      7078656     concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 29, 768)      9437952     concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 28, 768)      11797248    concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 31, 768)      3072        conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 30, 768)      3072        conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 29, 768)      3072        conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 28, 768)      3072        conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 31, 768)      0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 30, 768)      0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 29, 768)      0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 28, 768)      0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_18 (MaxPooling1D) (None, 10, 768)      0           activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_19 (MaxPooling1D) (None, 10, 768)      0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_20 (MaxPooling1D) (None, 9, 768)       0           activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_21 (MaxPooling1D) (None, 9, 768)       0           activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 38, 768)      0           max_pooling1d_18[0][0]           \n",
            "                                                                 max_pooling1d_19[0][0]           \n",
            "                                                                 max_pooling1d_20[0][0]           \n",
            "                                                                 max_pooling1d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 38, 768)      1770240     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 38, 768)      3072        conv1d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 38, 768)      0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_22 (MaxPooling1D) (None, 12, 768)      0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 12, 768)      1770240     max_pooling1d_22[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 12, 768)      3072        conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 12, 768)      0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 25088)        0           block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_23 (MaxPooling1D) (None, 4, 768)       0           activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "fc1 (Dense)                     (None, 4096)         102764544   flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 3072)         0           max_pooling1d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 49, 512)      0           block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "fc2 (Dense)                     (None, 4096)         16781312    fc1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_41 (Dense)                (None, 512)          1573376     flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_45 (Dense)                (None, 49, 32)       16416       reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_43 (Dense)                (None, 2048)         8390656     fc2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 512)          2048        dense_41[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 49, 32)       128         dense_45[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 2048)         8192        dense_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 512)          0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 49, 32)       0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 2048)         0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_159 (Dropout)           (None, 512)          0           activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_160 (Dropout)           (None, 2048)         0           activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_56 (Dense)                (None, 49, 32)       1024        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_57 (Dense)                (None, 49, 32)       1024        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_42 (Dense)                (None, 32)           16416       dropout_159[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_44 (Dense)                (None, 32)           65568       dropout_160[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot_14 (Dot)                    (None, 49, 49)       0           dense_56[0][0]                   \n",
            "                                                                 dense_57[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 32)           128         dense_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 32)           128         dense_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 49, 49)       0           dot_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 32)           0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 32)           0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 49, 49)       0           lambda_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_58 (Dense)                (None, 49, 32)       1024        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_47 (Dense)                (None, 32)           1024        activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_46 (Dense)                (None, 49, 32)       1024        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_51 (Dense)                (None, 32)           1024        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_52 (Dense)                (None, 4, 32)        24576       max_pooling1d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dot_15 (Dot)                    (None, 49, 32)       0           activation_65[0][0]              \n",
            "                                                                 dense_58[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_10 (Dot)                    (None, 49)           0           dense_47[0][0]                   \n",
            "                                                                 dense_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_12 (Dot)                    (None, 4)            0           dense_51[0][0]                   \n",
            "                                                                 dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_59 (Dense)                (None, 49, 32)       1056        dot_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 49)           0           dot_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 4)            0           dot_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 49, 32)       128         dense_59[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 49)           0           lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_48 (Dense)                (None, 49, 32)       1024        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 4)            0           lambda_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_53 (Dense)                (None, 4, 32)        24576       max_pooling1d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 49, 32)       0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dot_11 (Dot)                    (None, 32)           0           activation_59[0][0]              \n",
            "                                                                 dense_48[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_13 (Dot)                    (None, 32)           0           activation_62[0][0]              \n",
            "                                                                 dense_53[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_60 (Dense)                (None, 49, 32)       1056        activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_49 (Dense)                (None, 32)           1056        dot_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_54 (Dense)                (None, 32)           1056        dot_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 49, 32)       128         dense_60[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 32)           128         dense_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 32)           128         dense_54[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 49, 32)       0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 32)           0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 32)           0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_163 (Dropout)           (None, 49, 32)       0           activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_50 (Dense)                (None, 32)           1056        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_55 (Dense)                (None, 32)           1056        activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 49, 32)       0           activation_58[0][0]              \n",
            "                                                                 dropout_163[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 32)           128         dense_50[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 32)           128         dense_55[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_5 (LayerNor (None, 49, 32)       64          add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 32)           0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 32)           0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1568)         0           layer_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_161 (Dropout)           (None, 32)           0           activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_162 (Dropout)           (None, 32)           0           activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_61 (Dense)                (None, 32)           50208       flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32)           0           activation_55[0][0]              \n",
            "                                                                 dropout_161[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 32)           0           activation_57[0][0]              \n",
            "                                                                 dropout_162[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 32)           128         dense_61[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_3 (LayerNor (None, 32)           64          add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_4 (LayerNor (None, 32)           64          add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 32)           0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 160)          0           layer_normalization_3[0][0]      \n",
            "                                                                 layer_normalization_4[0][0]      \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_55[0][0]              \n",
            "                                                                 activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_164 (Dropout)           (None, 160)          0           concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_62 (Dense)                (None, 32)           5152        dropout_164[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 32)           128         dense_62[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 32)           0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_165 (Dropout)           (None, 32)           0           activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_63 (Dense)                (None, 1)            33          dropout_165[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 321,254,657\n",
            "Trainable params: 46,769,409\n",
            "Non-trainable params: 274,485,248\n",
            "__________________________________________________________________________________________________\n",
            "CPU times: user 4.91 s, sys: 441 ms, total: 5.35 s\n",
            "Wall time: 6.68 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4QPutEehi3D"
      },
      "source": [
        "model2.load_weights(os.path.join(base_dir, f'checkpoint-test/{MODEL_NAME}/cp-03.h5'))\n",
        "# model2.evaluate(X_val,y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CGnI1pI68Ek"
      },
      "source": [
        "#Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pol4YZ1x7Xie",
        "outputId": "dbde0ff7-d9d5-4cd4-a8ee-bd8d8ce1c58c"
      },
      "source": [
        "input_ids       = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in tqdm(test_sent):\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = MAX_LENGTH,         \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'np',   \n",
        "                        truncation = True,\n",
        "                   )\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  \n",
        "id_test_data        = np.concatenate(input_ids)\n",
        "mask_test_data      = np.concatenate(attention_masks)\n",
        "id_test_data.shape, mask_test_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1117 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1117/1117 [00:00<00:00, 6742.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1117, 32), (1117, 32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx6k6iXM82wF"
      },
      "source": [
        "X_test_data         = [\n",
        "    id_test_data,\n",
        "    mask_test_data,\n",
        "    img_test\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7wArOK-9HdO",
        "outputId": "fdcd33aa-798e-41c2-9b46-47f3fd866be2"
      },
      "source": [
        "pred = model2.predict(X_test_data, verbose=1)\n",
        "test_true = test_labels\n",
        "test_true.shape, pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 12s 224ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1117,), (1117, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHBbp1CqVElp"
      },
      "source": [
        "from sklearn import metrics\n",
        "theta = 0.4\n",
        "# Print the confusion matrix\n",
        "print(metrics.confusion_matrix(test_true,  (pred>theta).astype(int)))\n",
        "\n",
        "# Print the precision and recall, among other metrics\n",
        "\n",
        "print(metrics.classification_report(test_true, (pred>theta).astype(int), digits=3))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}